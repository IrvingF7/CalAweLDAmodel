{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = 'Overall' # The name of the data file. Now it is just for different language.\n",
    "num_of_representatives = 8 # The number of representative narrative you want to show for each topics.\n",
    "num_of_topics = 20 # The number of topics you want to generate from the data.\n",
    "countryList = ['Argentina','Australia','Austria','Brazil','Canada','Chile','China', 'France','Germany','India','Indonesia','Ireland','Japan','Korea','Mexico','Netherlands','Norway','Russia','Singapore','South_Africa','Spain','Sweden','Switzerland','Turkey','UK','USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For output\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk; \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud # Package by Andreas Mueller\n",
    "\n",
    "# General usage\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the output folder, according to the input data's language type\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "outputPath = 'output/' + dataName\n",
    "if not os.path.exists(outputPath):\n",
    "    os.mkdir(outputPath)\n",
    "outputPath = outputPath + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data's path, according to the input data's language type\n",
    "inputPath = 'input/' + dataName + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('English')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When I heard a boy sing Ave Maria perfectly. It was about three years ago. I was at the church in my cousin's baptism. I heard with awe how every note was sung with feeling, as if an angel was doing it. Later, I stayed still and was deeply moved by it\"\n",
      " 'I was at my house walking and suddenly sprained my ankle so abruptly that I thought I broke it. My reaction before this was exactly that expression. Once I reacted I slowly accomodated myself, and it ended up being nothing else than just the impression. Thank God it was nothing more than a sprain.'\n",
      " 'I was at the field walking with my husband when a big spider appeared. I was petrified and I wanted to run, but my body would not answer.'\n",
      " ...\n",
      " \"when i got paid the most money for clicking some random buttons was pretty cool. except for that part with the idiots didn't select the option to maximize everyones earnings and acted selfishly. how long does this take oh there we go bam.\"\n",
      " 'my nephew had to goto the hospital and when he got out he asked for me. so he called me as soon as he got out. i felt bad i couldnt be there and awe that i was the one he wanted when he got out'\n",
      " 'WHEN I SAW THIS BEAUTIFUL GIRL I WAS ABOUT TO BANG. I WAS AT HOUSE APARTMENT AND SHE HAD A DECENT FACE AND A PHAT ASS. AFTER WE TOOK OFF OUR CLOTHES WE BEGIN TO HAVE SEX. IT WAS ONE OF THE BEST EXPERIENCES OF MY LIFE.']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(inputPath + dataName + '_data.json', encoding = 'utf-8')\n",
    "print(df.Narrative.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When I heard a boy sing Ave Maria perfectly. It was about three years ago. I '\n",
      " 'was at the church in my cousins baptism. I heard with awe how every note was '\n",
      " 'sung with feeling, as if an angel was doing it. Later, I stayed still and '\n",
      " 'was deeply moved by it']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "#if df.X.values[0] == 'English':\n",
    "data = df.Narrative.values.tolist()\n",
    "#else:\n",
    "    #data = df.translation.values.tolist()\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when', 'heard', 'boy', 'sing', 'ave', 'maria', 'perfectly', 'it', 'was', 'about', 'three', 'years', 'ago', 'was', 'at', 'the', 'church', 'in', 'my', 'cousins', 'baptism', 'heard', 'with', 'awe', 'how', 'every', 'note', 'was', 'sung', 'with', 'feeling', 'as', 'if', 'an', 'angel', 'was', 'doing', 'it', 'later', 'stayed', 'still', 'and', 'was', 'deeply', 'moved', 'by', 'it']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\coding\\environment\\python3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'heard', 'boy', 'sing', 'ave', 'maria', 'perfectly', 'it', 'was', 'about', 'three', 'years_ago', 'was', 'at', 'the', 'church', 'in', 'my', 'cousins', 'baptism', 'heard', 'with', 'awe', 'how', 'every', 'note', 'was', 'sung', 'with', 'feeling', 'as', 'if', 'an', 'angel', 'was', 'doing', 'it', 'later', 'stayed', 'still', 'and', 'was', 'deeply', 'moved', 'by', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Build bigram and trigram\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hear', 'boy', 'sing', 'ave', 'maria', 'perfectly', 'years_ago', 'church', 'cousin', 'baptism', 'hear', 'awe', 'note', 'sing', 'feeling', 'angel', 'later', 'stay', 'still', 'deeply', 'moved']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "#getting bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1)]]\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_of_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.206*\"get\" + 0.035*\"show\" + 0.030*\"ago\" + 0.023*\"pregnant\" + 0.022*\"front\" '\n",
      "  '+ 0.016*\"excited\" + 0.015*\"call\" + 0.015*\"result\" + 0.013*\"celebrate\" + '\n",
      "  '0.012*\"issue\" + 0.012*\"afraid\" + 0.011*\"month\" + 0.011*\"couple\" + '\n",
      "  '0.010*\"favorite\" + 0.009*\"offer\"'),\n",
      " (1,\n",
      "  '0.068*\"moment\" + 0.063*\"life\" + 0.050*\"come\" + 0.040*\"look\" + '\n",
      "  '0.038*\"people\" + 0.038*\"know\" + 0.033*\"happen\" + 0.032*\"tell\" + '\n",
      "  '0.030*\"happy\" + 0.025*\"say\" + 0.023*\"person\" + 0.021*\"many\" + '\n",
      "  '0.017*\"nothing\" + 0.016*\"amazed\" + 0.015*\"face\"'),\n",
      " (2,\n",
      "  '0.080*\"world\" + 0.030*\"mom\" + 0.023*\"church\" + 0.019*\"bring\" + '\n",
      "  '0.019*\"strike\" + 0.018*\"light\" + 0.017*\"evening\" + 0.016*\"calm\" + '\n",
      "  '0.013*\"section\" + 0.012*\"sing\" + 0.011*\"guy\" + 0.011*\"sublimity\" + '\n",
      "  '0.011*\"delivery_room\" + 0.010*\"cousin\" + 0.010*\"red\"'),\n",
      " (3,\n",
      "  '0.123*\"feel\" + 0.080*\"awe\" + 0.053*\"want\" + 0.052*\"experience\" + '\n",
      "  '0.048*\"make\" + 0.038*\"take\" + 0.035*\"give\" + 0.034*\"think\" + 0.032*\"would\" '\n",
      "  '+ 0.021*\"way\" + 0.020*\"something\" + 0.019*\"thing\" + 0.019*\"great\" + '\n",
      "  '0.019*\"love\" + 0.016*\"visit\"'),\n",
      " (4,\n",
      "  '0.046*\"bear\" + 0.045*\"wife\" + 0.043*\"hospital\" + 0.040*\"son\" + 0.019*\"hold\" '\n",
      "  '+ 0.018*\"ever\" + 0.016*\"hour\" + 0.015*\"wonderful\" + 0.014*\"nature\" + '\n",
      "  '0.013*\"wait\" + 0.012*\"everyone\" + 0.012*\"girl\" + 0.011*\"sun\" + '\n",
      "  '0.011*\"partner\" + 0.010*\"awesome\"'),\n",
      " (5,\n",
      "  '0.122*\"work\" + 0.020*\"emotion\" + 0.020*\"couldnt_believe\" + 0.019*\"company\" '\n",
      "  '+ 0.019*\"area\" + 0.018*\"land\" + 0.017*\"point\" + 0.016*\"vehicle\" + '\n",
      "  '0.014*\"earth\" + 0.014*\"forever\" + 0.013*\"step\" + 0.013*\"reach\" + '\n",
      "  '0.012*\"wall\" + 0.010*\"mean\" + 0.009*\"sand\"'),\n",
      " (6,\n",
      "  '0.057*\"back\" + 0.052*\"big\" + 0.052*\"man\" + 0.042*\"little\" + 0.025*\"almost\" '\n",
      "  '+ 0.019*\"star\" + 0.019*\"sound\" + 0.018*\"state\" + 0.018*\"sky\" + 0.018*\"head\" '\n",
      "  '+ 0.017*\"street\" + 0.015*\"accident\" + 0.014*\"respect\" + 0.013*\"catch\" + '\n",
      "  '0.013*\"clear\"'),\n",
      " (7,\n",
      "  '0.034*\"jag\" + 0.014*\"och\" + 0.014*\"seat\" + 0.014*\"plane\" + 0.013*\"tear\" + '\n",
      "  '0.012*\"var\" + 0.012*\"det\" + 0.010*\"boy\" + 0.010*\"fly\" + 0.010*\"difficult\" + '\n",
      "  '0.010*\"med\" + 0.009*\"dog\" + 0.008*\"inte\" + 0.008*\"det_var\" + 0.008*\"hade\"'),\n",
      " (8,\n",
      "  '0.137*\"child\" + 0.118*\"birth\" + 0.075*\"daughter\" + 0.073*\"husband\" + '\n",
      "  '0.024*\"view\" + 0.022*\"min\" + 0.019*\"several\" + 0.016*\"climb\" + 0.015*\"law\" '\n",
      "  '+ 0.015*\"picture\" + 0.011*\"television\" + 0.011*\"relative\" + '\n",
      "  '0.010*\"grand_canyon\" + 0.010*\"realise\" + 0.008*\"marvel\"'),\n",
      " (9,\n",
      "  '0.068*\"car\" + 0.044*\"mountain\" + 0.032*\"drive\" + 0.026*\"top\" + 0.025*\"hit\" '\n",
      "  '+ 0.021*\"different\" + 0.017*\"animal\" + 0.017*\"explain\" + 0.015*\"air\" + '\n",
      "  '0.011*\"remain\" + 0.011*\"hole\" + 0.010*\"king\" + 0.008*\"driving\" + '\n",
      "  '0.008*\"kill\" + 0.008*\"terrible\"'),\n",
      " (10,\n",
      "  '0.121*\"go\" + 0.059*\"could\" + 0.051*\"home\" + 0.048*\"friend\" + 0.040*\"watch\" '\n",
      "  '+ 0.036*\"family\" + 0.030*\"alone\" + 0.026*\"years_ago\" + 0.024*\"find\" + '\n",
      "  '0.022*\"really\" + 0.018*\"long\" + 0.017*\"cry\" + 0.017*\"night\" + 0.016*\"fall\" '\n",
      "  '+ 0.014*\"surprise\"'),\n",
      " (11,\n",
      "  '0.147*\"not\" + 0.099*\"do\" + 0.030*\"hear\" + 0.029*\"situation\" + 0.025*\"have\" '\n",
      "  '+ 0.023*\"try\" + 0.015*\"mind\" + 0.014*\"summer\" + 0.013*\"course\" + '\n",
      "  '0.013*\"word\" + 0.011*\"busy\" + 0.011*\"describe\" + 0.011*\"probably\" + '\n",
      "  '0.010*\"experienced\" + 0.010*\"space\"'),\n",
      " (12,\n",
      "  '0.061*\"live\" + 0.049*\"everything\" + 0.042*\"also\" + 0.041*\"house\" + '\n",
      "  '0.034*\"change\" + 0.032*\"stop\" + 0.030*\"move\" + 0.027*\"school\" + 0.021*\"kid\" '\n",
      "  '+ 0.016*\"buy\" + 0.015*\"accept\" + 0.014*\"set\" + 0.013*\"student\" + '\n",
      "  '0.011*\"wedding\" + 0.010*\"health\"'),\n",
      " (13,\n",
      "  '0.040*\"joy\" + 0.038*\"money\" + 0.032*\"shock\" + 0.026*\"hand\" + 0.025*\"game\" + '\n",
      "  '0.019*\"phone\" + 0.018*\"high_school\" + 0.016*\"marry\" + 0.014*\"fast\" + '\n",
      "  '0.014*\"answer\" + 0.012*\"quickly\" + 0.012*\"finish\" + 0.010*\"else\" + '\n",
      "  '0.009*\"march\" + 0.009*\"savior\"'),\n",
      " (14,\n",
      "  '0.042*\"sit\" + 0.037*\"arrive\" + 0.037*\"trip\" + 0.028*\"still\" + 0.024*\"stay\" '\n",
      "  '+ 0.022*\"receive\" + 0.022*\"boyfriend\" + 0.020*\"memory\" + 0.019*\"pass\" + '\n",
      "  '0.019*\"finally\" + 0.018*\"incredible\" + 0.018*\"drink\" + 0.017*\"enjoy\" + '\n",
      "  '0.017*\"scared\" + 0.016*\"building\"'),\n",
      " (15,\n",
      "  '0.148*\"see\" + 0.123*\"time\" + 0.103*\"first\" + 0.056*\"day\" + 0.051*\"year\" + '\n",
      "  '0.040*\"good\" + 0.029*\"old\" + 0.027*\"amazing\" + 0.026*\"father\" + '\n",
      "  '0.023*\"baby\" + 0.021*\"beautiful\" + 0.020*\"mother\" + 0.014*\"become\" + '\n",
      "  '0.012*\"young\" + 0.011*\"brother\"'),\n",
      " (16,\n",
      "  '0.088*\"feeling\" + 0.050*\"meet\" + 0.044*\"last\" + 0.042*\"talk\" + '\n",
      "  '0.028*\"close\" + 0.027*\"eye\" + 0.023*\"strong\" + 0.017*\"side\" + '\n",
      "  '0.017*\"minute\" + 0.016*\"sense\" + 0.015*\"event\" + 0.015*\"speak\" + '\n",
      "  '0.015*\"meeting\" + 0.011*\"write\" + 0.011*\"lie\"'),\n",
      " (17,\n",
      "  '0.048*\"immediately\" + 0.041*\"afterwards\" + 0.031*\"job\" + 0.031*\"week\" + '\n",
      "  '0.028*\"hard\" + 0.021*\"high\" + 0.017*\"important\" + 0.015*\"hope\" + '\n",
      "  '0.014*\"huge\" + 0.012*\"position\" + 0.011*\"service\" + 0.011*\"less\" + '\n",
      "  '0.010*\"invite\" + 0.010*\"pride\" + 0.009*\"sadness\"'),\n",
      " (18,\n",
      "  '0.039*\"help\" + 0.037*\"later\" + 0.035*\"anything\" + 0.033*\"begin\" + '\n",
      "  '0.033*\"seem\" + 0.023*\"decide\" + 0.023*\"eat\" + 0.022*\"lose\" + 0.021*\"next\" + '\n",
      "  '0.019*\"concert\" + 0.017*\"day\" + 0.016*\"song\" + 0.016*\"need\" + 0.012*\"pa\" + '\n",
      "  '0.011*\"voice\"'),\n",
      " (19,\n",
      "  '0.054*\"remember\" + 0.054*\"never\" + 0.046*\"walk\" + 0.038*\"new\" + '\n",
      "  '0.035*\"place\" + 0.027*\"can\" + 0.026*\"lot\" + 0.026*\"always\" + 0.025*\"month\" '\n",
      "  '+ 0.020*\"water\" + 0.020*\"fear\" + 0.019*\"may\" + 0.018*\"sea\" + 0.017*\"city\" + '\n",
      "  '0.015*\"holiday\"')]\n"
     ]
    }
   ],
   "source": [
    "num_of_words_per_topic = 15\n",
    "topics = lda_model.print_topics(num_of_topics, num_of_words_per_topic)\n",
    "doc_lda = lda_model[corpus]\n",
    "pprint(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4097896212154234\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'C:/mallet-2.0.8/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_of_topics, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3326544986605814\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.060*\"years_ago\" + 0.051*\"visit\" + 0.048*\"amazing\" + 0.043*\"mountain\" + '\n",
      "  '0.040*\"trip\" + 0.028*\"city\" + 0.027*\"travel\" + 0.025*\"view\" + '\n",
      "  '0.024*\"country\" + 0.023*\"top\" + 0.018*\"enjoy\" + 0.016*\"boyfriend\" + '\n",
      "  '0.016*\"cold\" + 0.015*\"climb\" + 0.015*\"building\"'),\n",
      " (1,\n",
      "  '0.174*\"experience\" + 0.101*\"time\" + 0.074*\"feeling\" + 0.031*\"sit\" + '\n",
      "  '0.027*\"intense\" + 0.026*\"strong\" + 0.021*\"event\" + 0.020*\"church\" + '\n",
      "  '0.020*\"emotion\" + 0.020*\"begin\" + 0.018*\"tear\" + 0.015*\"kid\" + 0.013*\"pray\" '\n",
      "  '+ 0.011*\"describe\" + 0.011*\"write\"'),\n",
      " (2,\n",
      "  '0.056*\"hear\" + 0.046*\"man\" + 0.040*\"news\" + 0.029*\"jag\" + 0.026*\"bad\" + '\n",
      "  '0.019*\"memory\" + 0.017*\"future\" + 0.015*\"meeting\" + 0.015*\"min\" + '\n",
      "  '0.013*\"sad\" + 0.012*\"och\" + 0.010*\"listen\" + 0.010*\"det\" + 0.009*\"var\" + '\n",
      "  '0.009*\"internet\"'),\n",
      " (3,\n",
      "  '0.193*\"time\" + 0.114*\"year\" + 0.093*\"happen\" + 0.086*\"good\" + 0.045*\"thing\" '\n",
      "  '+ 0.039*\"meet\" + 0.020*\"partner\" + 0.016*\"nice\" + 0.014*\"boy\" + '\n",
      "  '0.013*\"high_school\" + 0.012*\"study\" + 0.012*\"mine\" + 0.011*\"passed_away\" + '\n",
      "  '0.010*\"couple\" + 0.007*\"home\"'),\n",
      " (4,\n",
      "  '0.048*\"house\" + 0.043*\"night\" + 0.035*\"husband\" + 0.031*\"hour\" + '\n",
      "  '0.030*\"close\" + 0.024*\"light\" + 0.022*\"walk\" + 0.022*\"star\" + '\n",
      "  '0.021*\"morning\" + 0.019*\"minute\" + 0.019*\"sleep\" + 0.019*\"fire\" + '\n",
      "  '0.018*\"fly\" + 0.018*\"stay\" + 0.017*\"point\"'),\n",
      " (5,\n",
      "  '0.124*\"day\" + 0.070*\"mother\" + 0.051*\"father\" + 0.030*\"family\" + '\n",
      "  '0.030*\"end\" + 0.029*\"young\" + 0.027*\"realize\" + 0.026*\"die\" + '\n",
      "  '0.024*\"brother\" + 0.022*\"money\" + 0.020*\"death\" + 0.018*\"law\" + '\n",
      "  '0.017*\"pain\" + 0.014*\"age\" + 0.014*\"due\"'),\n",
      " (6,\n",
      "  '0.058*\"beautiful\" + 0.033*\"wonderful\" + 0.032*\"world\" + 0.022*\"sea\" + '\n",
      "  '0.022*\"beauty\" + 0.020*\"sun\" + 0.018*\"holiday\" + 0.018*\"wife\" + '\n",
      "  '0.017*\"landscape\" + 0.015*\"head\" + 0.015*\"calm\" + 0.015*\"plane\" + '\n",
      "  '0.013*\"beach\" + 0.013*\"incredible\" + 0.013*\"sky\"'),\n",
      " (7,\n",
      "  '0.263*\"feel\" + 0.057*\"person\" + 0.043*\"world\" + 0.033*\"god\" + 0.029*\"thing\" '\n",
      "  '+ 0.028*\"eye\" + 0.026*\"amazed\" + 0.023*\"understand\" + 0.022*\"make\" + '\n",
      "  '0.021*\"body\" + 0.019*\"special\" + 0.016*\"happiness\" + 0.016*\"mind\" + '\n",
      "  '0.016*\"important\" + 0.012*\"film\"'),\n",
      " (8,\n",
      "  '0.135*\"friend\" + 0.125*\"home\" + 0.086*\"watch\" + 0.068*\"family\" + 0.036*\"tv\" '\n",
      "  '+ 0.029*\"amazed\" + 0.025*\"girl\" + 0.021*\"show\" + 0.018*\"thought\" + '\n",
      "  '0.014*\"attack\" + 0.011*\"space\" + 0.011*\"notice\" + 0.010*\"television\" + '\n",
      "  '0.009*\"member\" + 0.007*\"performance\"'),\n",
      " (9,\n",
      "  '0.091*\"give\" + 0.068*\"happy\" + 0.061*\"love\" + 0.056*\"find\" + 0.029*\"doctor\" '\n",
      "  '+ 0.025*\"pregnant\" + 0.022*\"learn\" + 0.022*\"hug\" + 0.019*\"bring\" + '\n",
      "  '0.016*\"finally\" + 0.016*\"heart\" + 0.015*\"happiness\" + 0.014*\"buy\" + '\n",
      "  '0.014*\"year\" + 0.013*\"kiss\"'),\n",
      " (10,\n",
      "  '0.076*\"remember\" + 0.042*\"parent\" + 0.039*\"talk\" + 0.038*\"lot\" + '\n",
      "  '0.035*\"room\" + 0.031*\"cry\" + 0.027*\"wait\" + 0.025*\"sister\" + 0.023*\"mom\" + '\n",
      "  '0.021*\"surprised\" + 0.019*\"concert\" + 0.016*\"group\" + 0.015*\"dinner\" + '\n",
      "  '0.015*\"song\" + 0.014*\"speak\"'),\n",
      " (11,\n",
      "  '0.178*\"life\" + 0.114*\"people\" + 0.071*\"live\" + 0.062*\"great\" + '\n",
      "  '0.034*\"reverence\" + 0.030*\"change\" + 0.019*\"respect\" + 0.018*\"awesome\" + '\n",
      "  '0.017*\"put\" + 0.010*\"encounter\" + 0.009*\"heart\" + 0.008*\"deep\" + '\n",
      "  '0.008*\"story\" + 0.008*\"deeply\" + 0.008*\"university\"'),\n",
      " (12,\n",
      "  '0.053*\"make\" + 0.045*\"situation\" + 0.045*\"big\" + 0.042*\"stand\" + '\n",
      "  '0.023*\"open\" + 0.023*\"girlfriend\" + 0.021*\"hand\" + 0.021*\"completely\" + '\n",
      "  '0.020*\"meet\" + 0.017*\"kind\" + 0.017*\"word\" + 0.014*\"recall\" + '\n",
      "  '0.014*\"afraid\" + 0.014*\"horse\" + 0.013*\"difficult\"'),\n",
      " (13,\n",
      "  '0.055*\"car\" + 0.044*\"call\" + 0.028*\"school\" + 0.024*\"front\" + 0.024*\"drive\" '\n",
      "  '+ 0.021*\"arrive\" + 0.021*\"leave\" + 0.018*\"hit\" + 0.017*\"accident\" + '\n",
      "  '0.017*\"exam\" + 0.017*\"street\" + 0.015*\"turn\" + 0.015*\"student\" + '\n",
      "  '0.013*\"complete\" + 0.013*\"side\"'),\n",
      " (14,\n",
      "  '0.050*\"make\" + 0.038*\"long\" + 0.035*\"ago\" + 0.030*\"play\" + 0.028*\"job\" + '\n",
      "  '0.026*\"month\" + 0.021*\"read\" + 0.019*\"team\" + 0.018*\"game\" + 0.018*\"week\" + '\n",
      "  '0.017*\"match\" + 0.017*\"full\" + 0.016*\"win\" + 0.015*\"celebrate\" + '\n",
      "  '0.012*\"final\"'),\n",
      " (15,\n",
      "  '0.110*\"child\" + 0.105*\"birth\" + 0.079*\"son\" + 0.068*\"daughter\" + '\n",
      "  '0.065*\"hospital\" + 0.064*\"bear\" + 0.057*\"wife\" + 0.037*\"baby\" + '\n",
      "  '0.035*\"husband\" + 0.032*\"hold\" + 0.027*\"joy\" + 0.018*\"arm\" + 0.015*\"give\" + '\n",
      "  '0.013*\"present\" + 0.011*\"perfect\"'),\n",
      " (16,\n",
      "  '0.050*\"start\" + 0.039*\"suddenly\" + 0.038*\"fall\" + 0.029*\"move\" + '\n",
      "  '0.029*\"water\" + 0.029*\"fear\" + 0.023*\"run\" + 0.017*\"woman\" + 0.017*\"animal\" '\n",
      "  '+ 0.016*\"jump\" + 0.016*\"high\" + 0.014*\"meter\" + 0.014*\"cat\" + 0.013*\"train\" '\n",
      "  '+ 0.012*\"scene\"'),\n",
      " (17,\n",
      "  '0.223*\"awe\" + 0.168*\"feel\" + 0.040*\"nature\" + 0.039*\"small\" + 0.025*\"sense\" '\n",
      "  '+ 0.024*\"human\" + 0.017*\"power\" + 0.013*\"movie\" + 0.012*\"strike\" + '\n",
      "  '0.010*\"even_though\" + 0.010*\"inspire\" + 0.010*\"explain\" + 0.009*\"fill\" + '\n",
      "  '0.008*\"catch\" + 0.008*\"extremely\"'),\n",
      " (18,\n",
      "  '0.109*\"work\" + 0.041*\"walk\" + 0.028*\"leave\" + 0.026*\"pass\" + 0.022*\"lose\" + '\n",
      "  '0.022*\"receive\" + 0.019*\"decide\" + 0.018*\"colleague\" + 0.018*\"hard\" + '\n",
      "  '0.016*\"company\" + 0.016*\"stop\" + 0.014*\"continue\" + 0.013*\"phone\" + '\n",
      "  '0.013*\"bit\" + 0.011*\"break\"'),\n",
      " (19,\n",
      "  '0.169*\"moment\" + 0.050*\"place\" + 0.040*\"immediately\" + 0.038*\"back\" + '\n",
      "  '0.037*\"surprise\" + 0.029*\"face\" + 0.025*\"shock\" + 0.025*\"expect\" + '\n",
      "  '0.024*\"result\" + 0.022*\"make\" + 0.019*\"sensation\" + 0.014*\"forget\" + '\n",
      "  '0.013*\"discover\" + 0.011*\"reach\" + 0.010*\"hope\"')]\n"
     ]
    }
   ],
   "source": [
    "topics = ldamallet.print_topics(num_of_topics, num_of_words_per_topic)\n",
    "pprint(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '0.060', 'years_ago', '0.051', 'visit', '0.048', 'amazing', '0.043', 'mountain', '0.040', 'trip', '0.028', 'city', '0.027', 'travel', '0.025', 'view', '0.024', 'country', '0.023', 'top', '0.018', 'enjoy', '0.016', 'boyfriend', '0.016', 'cold', '0.015', 'climb', '0.015', 'building'], ['1', '0.174', 'experience', '0.101', 'time', '0.074', 'feeling', '0.031', 'sit', '0.027', 'intense', '0.026', 'strong', '0.021', 'event', '0.020', 'church', '0.020', 'emotion', '0.020', 'begin', '0.018', 'tear', '0.015', 'kid', '0.013', 'pray', '0.011', 'describe', '0.011', 'write'], ['2', '0.056', 'hear', '0.046', 'man', '0.040', 'news', '0.029', 'jag', '0.026', 'bad', '0.019', 'memory', '0.017', 'future', '0.015', 'meeting', '0.015', 'min', '0.013', 'sad', '0.012', 'och', '0.010', 'listen', '0.010', 'det', '0.009', 'var', '0.009', 'internet'], ['3', '0.193', 'time', '0.114', 'year', '0.093', 'happen', '0.086', 'good', '0.045', 'thing', '0.039', 'meet', '0.020', 'partner', '0.016', 'nice', '0.014', 'boy', '0.013', 'high_school', '0.012', 'study', '0.012', 'mine', '0.011', 'passed_away', '0.010', 'couple', '0.007', 'home'], ['4', '0.048', 'house', '0.043', 'night', '0.035', 'husband', '0.031', 'hour', '0.030', 'close', '0.024', 'light', '0.022', 'walk', '0.022', 'star', '0.021', 'morning', '0.019', 'minute', '0.019', 'sleep', '0.019', 'fire', '0.018', 'fly', '0.018', 'stay', '0.017', 'point'], ['5', '0.124', 'day', '0.070', 'mother', '0.051', 'father', '0.030', 'family', '0.030', 'end', '0.029', 'young', '0.027', 'realize', '0.026', 'die', '0.024', 'brother', '0.022', 'money', '0.020', 'death', '0.018', 'law', '0.017', 'pain', '0.014', 'age', '0.014', 'due'], ['6', '0.058', 'beautiful', '0.033', 'wonderful', '0.032', 'world', '0.022', 'sea', '0.022', 'beauty', '0.020', 'sun', '0.018', 'holiday', '0.018', 'wife', '0.017', 'landscape', '0.015', 'head', '0.015', 'calm', '0.015', 'plane', '0.013', 'beach', '0.013', 'incredible', '0.013', 'sky'], ['7', '0.263', 'feel', '0.057', 'person', '0.043', 'world', '0.033', 'god', '0.029', 'thing', '0.028', 'eye', '0.026', 'amazed', '0.023', 'understand', '0.022', 'make', '0.021', 'body', '0.019', 'special', '0.016', 'happiness', '0.016', 'mind', '0.016', 'important', '0.012', 'film'], ['8', '0.135', 'friend', '0.125', 'home', '0.086', 'watch', '0.068', 'family', '0.036', 'tv', '0.029', 'amazed', '0.025', 'girl', '0.021', 'show', '0.018', 'thought', '0.014', 'attack', '0.011', 'space', '0.011', 'notice', '0.010', 'television', '0.009', 'member', '0.007', 'performance'], ['9', '0.091', 'give', '0.068', 'happy', '0.061', 'love', '0.056', 'find', '0.029', 'doctor', '0.025', 'pregnant', '0.022', 'learn', '0.022', 'hug', '0.019', 'bring', '0.016', 'finally', '0.016', 'heart', '0.015', 'happiness', '0.014', 'buy', '0.014', 'year', '0.013', 'kiss'], ['10', '0.076', 'remember', '0.042', 'parent', '0.039', 'talk', '0.038', 'lot', '0.035', 'room', '0.031', 'cry', '0.027', 'wait', '0.025', 'sister', '0.023', 'mom', '0.021', 'surprised', '0.019', 'concert', '0.016', 'group', '0.015', 'dinner', '0.015', 'song', '0.014', 'speak'], ['11', '0.178', 'life', '0.114', 'people', '0.071', 'live', '0.062', 'great', '0.034', 'reverence', '0.030', 'change', '0.019', 'respect', '0.018', 'awesome', '0.017', 'put', '0.010', 'encounter', '0.009', 'heart', '0.008', 'deep', '0.008', 'story', '0.008', 'deeply', '0.008', 'university'], ['12', '0.053', 'make', '0.045', 'situation', '0.045', 'big', '0.042', 'stand', '0.023', 'open', '0.023', 'girlfriend', '0.021', 'hand', '0.021', 'completely', '0.020', 'meet', '0.017', 'kind', '0.017', 'word', '0.014', 'recall', '0.014', 'afraid', '0.014', 'horse', '0.013', 'difficult'], ['13', '0.055', 'car', '0.044', 'call', '0.028', 'school', '0.024', 'front', '0.024', 'drive', '0.021', 'arrive', '0.021', 'leave', '0.018', 'hit', '0.017', 'accident', '0.017', 'exam', '0.017', 'street', '0.015', 'turn', '0.015', 'student', '0.013', 'complete', '0.013', 'side'], ['14', '0.050', 'make', '0.038', 'long', '0.035', 'ago', '0.030', 'play', '0.028', 'job', '0.026', 'month', '0.021', 'read', '0.019', 'team', '0.018', 'game', '0.018', 'week', '0.017', 'match', '0.017', 'full', '0.016', 'win', '0.015', 'celebrate', '0.012', 'final'], ['15', '0.110', 'child', '0.105', 'birth', '0.079', 'son', '0.068', 'daughter', '0.065', 'hospital', '0.064', 'bear', '0.057', 'wife', '0.037', 'baby', '0.035', 'husband', '0.032', 'hold', '0.027', 'joy', '0.018', 'arm', '0.015', 'give', '0.013', 'present', '0.011', 'perfect'], ['16', '0.050', 'start', '0.039', 'suddenly', '0.038', 'fall', '0.029', 'move', '0.029', 'water', '0.029', 'fear', '0.023', 'run', '0.017', 'woman', '0.017', 'animal', '0.016', 'jump', '0.016', 'high', '0.014', 'meter', '0.014', 'cat', '0.013', 'train', '0.012', 'scene'], ['17', '0.223', 'awe', '0.168', 'feel', '0.040', 'nature', '0.039', 'small', '0.025', 'sense', '0.024', 'human', '0.017', 'power', '0.013', 'movie', '0.012', 'strike', '0.010', 'even_though', '0.010', 'inspire', '0.010', 'explain', '0.009', 'fill', '0.008', 'catch', '0.008', 'extremely'], ['18', '0.109', 'work', '0.041', 'walk', '0.028', 'leave', '0.026', 'pass', '0.022', 'lose', '0.022', 'receive', '0.019', 'decide', '0.018', 'colleague', '0.018', 'hard', '0.016', 'company', '0.016', 'stop', '0.014', 'continue', '0.013', 'phone', '0.013', 'bit', '0.011', 'break'], ['19', '0.169', 'moment', '0.050', 'place', '0.040', 'immediately', '0.038', 'back', '0.037', 'surprise', '0.029', 'face', '0.025', 'shock', '0.025', 'expect', '0.024', 'result', '0.022', 'make', '0.019', 'sensation', '0.014', 'forget', '0.013', 'discover', '0.011', 'reach', '0.010', 'hope']]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the keywords and percentages for futrue useage\n",
    "# allKeywords[i] will give a array of keywords for topic i\n",
    "# allPercentages[i] will give a array of percentages for topic i\n",
    "index = 0\n",
    "chunks = [None] * num_of_topics\n",
    "allKeywords = [None] * num_of_topics\n",
    "allPercentages = [None] * num_of_topics\n",
    "for chunk in topics:\n",
    "    chunk = chunk[1]\n",
    "    percentages = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", chunk) #credit to miku on Stackoverflow\n",
    "    keywords = re.findall('\"([^\"]*)\"', chunk) #credit to jspcal on Stackoverflow\n",
    "    allKeywords[index] = keywords\n",
    "    allPercentages[index] = percentages\n",
    "    result = [None] * 2 * num_of_words_per_topic\n",
    "    result[::2] = percentages\n",
    "    result[1::2] = keywords\n",
    "    result = [str(index)] + result\n",
    "    chunks[index] = result # A array stroing arrays of keywords and corresponding percentages\n",
    "    index += 1\n",
    "print(chunks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['years_ago', 'visit', 'amazing', 'mountain', 'trip', 'city', 'travel', 'view', 'country', 'top', 'enjoy', 'boyfriend', 'cold', 'climb', 'building']\n",
      "['0.060', '0.051', '0.048', '0.043', '0.040', '0.028', '0.027', '0.025', '0.024', '0.023', '0.018', '0.016', '0.016', '0.015', '0.015']\n"
     ]
    }
   ],
   "source": [
    "print(allKeywords[0])\n",
    "print(allPercentages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [None] * 2 * num_of_words_per_topic\n",
    "\n",
    "for i in range(0, 2 * num_of_words_per_topic):\n",
    "    if i % 2 == 0:\n",
    "               header[i] = 'Percentage'\n",
    "    else:\n",
    "               header[i] = 'Keyword'\n",
    "\n",
    "header = ['Topic No.'] + header\n",
    "topicsPath = outputPath + dataName + '_' + 'topics.csv'\n",
    "with open(topicsPath, 'w', newline='', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([header])\n",
    "    for chunk in chunks:\n",
    "        writer.writerows([chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the most dominant topic for each narrative\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data)\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Narrative No.', 'Dominant_Topic', 'Topic Contribution', 'Keywords', 'Text']\n",
    "\n",
    "dominantTopicsPath = outputPath + dataName + '_' + 'dominant_topic.csv'\n",
    "df_dominant_topic.to_csv(dominantTopicsPath, encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Topic No.  Dominant_Topic  Topic Contribution  \\\n",
      "0             0             2.0              0.1200   \n",
      "1             1            16.0              0.1137   \n",
      "2             2            12.0              0.0800   \n",
      "3             3             6.0              0.1468   \n",
      "4             4             5.0              0.0926   \n",
      "5             5             4.0              0.1005   \n",
      "6             6            10.0              0.0850   \n",
      "7             7            15.0              0.1241   \n",
      "8             8             9.0              0.0944   \n",
      "9             9            10.0              0.0932   \n",
      "10           10            16.0              0.0954   \n",
      "11           11             8.0              0.0859   \n",
      "12           12             3.0              0.0708   \n",
      "13           13             0.0              0.0744   \n",
      "14           14             3.0              0.0770   \n",
      "15           15            16.0              0.1042   \n",
      "16           16            19.0              0.0773   \n",
      "17           17             8.0              0.0763   \n",
      "18           18             5.0              0.1289   \n",
      "19           19            18.0              0.1055   \n",
      "20           20             1.0              0.0636   \n",
      "21           21            15.0              0.1460   \n",
      "22           22            18.0              0.1431   \n",
      "23           23             0.0              0.0770   \n",
      "24           24             0.0              0.0761   \n",
      "25           25             3.0              0.1117   \n",
      "26           26            14.0              0.1032   \n",
      "27           27            16.0              0.0884   \n",
      "28           28            15.0              0.1641   \n",
      "29           29             8.0              0.0942   \n",
      "...         ...             ...                 ...   \n",
      "2734       2734            15.0              0.1019   \n",
      "2735       2735             6.0              0.0788   \n",
      "2736       2736            15.0              0.0789   \n",
      "2737       2737            13.0              0.1101   \n",
      "2738       2738             8.0              0.0833   \n",
      "2739       2739             1.0              0.0977   \n",
      "2740       2740            15.0              0.0913   \n",
      "2741       2741            13.0              0.2756   \n",
      "2742       2742            16.0              0.1078   \n",
      "2743       2743             4.0              0.0869   \n",
      "2744       2744            18.0              0.1047   \n",
      "2745       2745             4.0              0.0880   \n",
      "2746       2746             0.0              0.1172   \n",
      "2747       2747            12.0              0.0697   \n",
      "2748       2748             1.0              0.0769   \n",
      "2749       2749            14.0              0.0860   \n",
      "2750       2750            15.0              0.0846   \n",
      "2751       2751            12.0              0.1088   \n",
      "2752       2752            10.0              0.0983   \n",
      "2753       2753            15.0              0.0782   \n",
      "2754       2754             5.0              0.0818   \n",
      "2755       2755             8.0              0.0744   \n",
      "2756       2756            14.0              0.1187   \n",
      "2757       2757            13.0              0.1271   \n",
      "2758       2758            12.0              0.0965   \n",
      "2759       2759            14.0              0.0887   \n",
      "2760       2760             7.0              0.0673   \n",
      "2761       2761            15.0              0.0755   \n",
      "2762       2762             7.0              0.0692   \n",
      "2763       2763            14.0              0.0709   \n",
      "\n",
      "                                               Keywords  \\\n",
      "0     hear, man, news, jag, bad, memory, future, mee...   \n",
      "1     start, suddenly, fall, move, fear, water, run,...   \n",
      "2     make, big, situation, stand, open, girlfriend,...   \n",
      "3     beautiful, wonderful, world, sea, beauty, sun,...   \n",
      "4     day, mother, father, family, end, young, reali...   \n",
      "5     house, night, husband, hour, close, light, wal...   \n",
      "6     remember, parent, talk, lot, room, cry, wait, ...   \n",
      "7     child, birth, son, daughter, hospital, bear, w...   \n",
      "8     give, happy, love, find, doctor, pregnant, lea...   \n",
      "9     remember, parent, talk, lot, room, cry, wait, ...   \n",
      "10    start, suddenly, fall, move, fear, water, run,...   \n",
      "11    friend, home, watch, family, tv, amazed, girl,...   \n",
      "12    time, year, happen, good, thing, meet, partner...   \n",
      "13    years_ago, visit, amazing, mountain, trip, cit...   \n",
      "14    time, year, happen, good, thing, meet, partner...   \n",
      "15    start, suddenly, fall, move, fear, water, run,...   \n",
      "16    moment, place, immediately, back, surprise, fa...   \n",
      "17    friend, home, watch, family, tv, amazed, girl,...   \n",
      "18    day, mother, father, family, end, young, reali...   \n",
      "19    work, walk, leave, pass, receive, lose, decide...   \n",
      "20    experience, time, feeling, sit, intense, stron...   \n",
      "21    child, birth, son, daughter, hospital, bear, w...   \n",
      "22    work, walk, leave, pass, receive, lose, decide...   \n",
      "23    years_ago, visit, amazing, mountain, trip, cit...   \n",
      "24    years_ago, visit, amazing, mountain, trip, cit...   \n",
      "25    time, year, happen, good, thing, meet, partner...   \n",
      "26    make, long, ago, play, job, month, read, team,...   \n",
      "27    start, suddenly, fall, move, fear, water, run,...   \n",
      "28    child, birth, son, daughter, hospital, bear, w...   \n",
      "29    friend, home, watch, family, tv, amazed, girl,...   \n",
      "...                                                 ...   \n",
      "2734  child, birth, son, daughter, hospital, bear, w...   \n",
      "2735  beautiful, wonderful, world, sea, beauty, sun,...   \n",
      "2736  child, birth, son, daughter, hospital, bear, w...   \n",
      "2737  car, call, school, front, drive, leave, arrive...   \n",
      "2738  friend, home, watch, family, tv, amazed, girl,...   \n",
      "2739  experience, time, feeling, sit, intense, stron...   \n",
      "2740  child, birth, son, daughter, hospital, bear, w...   \n",
      "2741  car, call, school, front, drive, leave, arrive...   \n",
      "2742  start, suddenly, fall, move, fear, water, run,...   \n",
      "2743  house, night, husband, hour, close, light, wal...   \n",
      "2744  work, walk, leave, pass, receive, lose, decide...   \n",
      "2745  house, night, husband, hour, close, light, wal...   \n",
      "2746  years_ago, visit, amazing, mountain, trip, cit...   \n",
      "2747  make, big, situation, stand, open, girlfriend,...   \n",
      "2748  experience, time, feeling, sit, intense, stron...   \n",
      "2749  make, long, ago, play, job, month, read, team,...   \n",
      "2750  child, birth, son, daughter, hospital, bear, w...   \n",
      "2751  make, big, situation, stand, open, girlfriend,...   \n",
      "2752  remember, parent, talk, lot, room, cry, wait, ...   \n",
      "2753  child, birth, son, daughter, hospital, bear, w...   \n",
      "2754  day, mother, father, family, end, young, reali...   \n",
      "2755  friend, home, watch, family, tv, amazed, girl,...   \n",
      "2756  make, long, ago, play, job, month, read, team,...   \n",
      "2757  car, call, school, front, drive, leave, arrive...   \n",
      "2758  make, big, situation, stand, open, girlfriend,...   \n",
      "2759  make, long, ago, play, job, month, read, team,...   \n",
      "2760  feel, person, world, god, thing, eye, amazed, ...   \n",
      "2761  child, birth, son, daughter, hospital, bear, w...   \n",
      "2762  feel, person, world, god, thing, eye, amazed, ...   \n",
      "2763  make, long, ago, play, job, month, read, team,...   \n",
      "\n",
      "                                                   Text  \n",
      "0     When I heard a boy sing Ave Maria perfectly. I...  \n",
      "1     I was at my house walking and suddenly spraine...  \n",
      "2     I was at the field walking with my husband whe...  \n",
      "3     I was astonished by watching the moon show up ...  \n",
      "4     1 My first son. 2º It was in October 1992. 3º ...  \n",
      "5     It made me feel I was in danger, it was at nig...  \n",
      "6     What made me feel that way was the moment I wa...  \n",
      "7     The birth of my first son. / This happened whe...  \n",
      "8     What: finding a person I did not see since a c...  \n",
      "9     Tandil, Indio Solaris gig, the worst was confi...  \n",
      "10    One time I had gone fishing when a fish jumped...  \n",
      "11    When I found out that my best friend was batte...  \n",
      "12    I was in an amusement park with a group of fri...  \n",
      "13    My couple was cheating on me. Two years ago. I...  \n",
      "14    I was studying when my couple told me she was ...  \n",
      "15    It happened when I was a teenager, in a trip t...  \n",
      "16    A declaration about a sentimental situation of...  \n",
      "17    I saw the boyfriend of a friend kissing with a...  \n",
      "18    We were reunited with some friends, like every...  \n",
      "19    I can mention three or four situations about i...  \n",
      "20    Yes, when I began working at Cordobas municipa...  \n",
      "21    The birth of my first son / 21 years ago / At ...  \n",
      "22    Many instances amaze me. The most important on...  \n",
      "23    An image. Six years ago. I was lying down. My ...  \n",
      "24    It was in the year 2009, I went on holidays wi...  \n",
      "25    I had read a book on esotericism about soul tr...  \n",
      "26    It happened when I was offered to conduct a TV...  \n",
      "27    When I found out about my pregnancy. It happen...  \n",
      "28    It was the instant I saw my grandson for the f...  \n",
      "29    I was at home, an important notice about a fam...  \n",
      "...                                                 ...  \n",
      "2734  WITNESSING THE MIRACLE OF MY FOUR CHILDREN BEI...  \n",
      "2735  I sailed my 42 ft Catalina Sailboat from San D...  \n",
      "2736  When I got my first standard poodle, My wife g...  \n",
      "2737  when I witnessed the world trade center being ...  \n",
      "2738  According to the picture above, it was 9/11. I...  \n",
      "2739  Went to Kenny Space Center and oserved Apollo ...  \n",
      "2740  The birth of my 1st child. Was with my wife. I...  \n",
      "2741  I was a paramedic in Portland Oregon. In 1979 ...  \n",
      "2742  i was standing and noticed something that look...  \n",
      "2743  I experience \"awe\" on a regular basis. Every t...  \n",
      "2744  When my dad was diagnosed with cancer, and hav...  \n",
      "2745  Summer camp as a child 9 years old. Camp was n...  \n",
      "2746  I felt awe when I visited the Smoky Mountains ...  \n",
      "2747  I ran a really fast time on the track without ...  \n",
      "2748  Arriving home to find my fiance had taken her ...  \n",
      "2749  my \"awe moment, I feel like I get a lot of tho...  \n",
      "2750  I was in most awe when my first child was born...  \n",
      "2751  My cat was attacked and her leg was bitten and...  \n",
      "2752  I was visiting my uncles house for dinner. As ...  \n",
      "2753  When my brother was born. It was one of the ha...  \n",
      "2754  the finding of my father in my library when he...  \n",
      "2755  I was 13 in 1969 watching a man land on the mo...  \n",
      "2756  1973 at my cousins restaurant. My Father worke...  \n",
      "2757  IT WAS RIGHT ASTER MY FATHER HAD DIED. I BECAM...  \n",
      "2758  Honestly i cant recall such situation. The sec...  \n",
      "2759  I called the \"LARRY KING SHOW\" thirty times. I...  \n",
      "2760  nothing comes to mind. lkhllkasgkggggggggggggg...  \n",
      "2761  when i got paid the most money for clicking so...  \n",
      "2762  my nephew had to goto the hospital and when he...  \n",
      "2763  WHEN I SAW THIS BEAUTIFUL GIRL I WAS ABOUT TO ...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2764 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating top 'num_of_representatives' most representative narratives\n",
    "import operator\n",
    "repre = [None] * num_of_topics\n",
    "for i, row in enumerate(ldamallet[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if repre[topic_num] is None:\n",
    "                repre[topic_num] = dict()\n",
    "            repre[topic_num][str(i)] = prop_topic\n",
    "sorted_repre = [None] * num_of_topics\n",
    "for i in range(0, len(repre)):\n",
    "    sorted_repre[i] = sorted(repre[i].items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "#pprint(sorted_repre)\n",
    "allTheTopics = [None] * num_of_topics\n",
    "\n",
    "index = 0\n",
    "for item in sorted_repre:\n",
    "    allTheTopics[index] = [index]\n",
    "    allTheTopics[index] += [allKeywords[index]]\n",
    "    for i in range(0, num_of_representatives):\n",
    "        allTheTopics[index] += [data[int(sorted_repre[index][i][0])]]\n",
    "    index += 1\n",
    "representativeNarraPath = outputPath + dataName + '_' + 'representative_narratives.csv'\n",
    "header = (2 + num_of_representatives) * [None]\n",
    "header[0] = 'Topic No.'\n",
    "header[1] = 'Keywords'\n",
    "for i in range(1, num_of_representatives+1):\n",
    "    header[i + 1] = i\n",
    "with open(representativeNarraPath, 'w', newline='', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([header])\n",
    "    for topic in allTheTopics:\n",
    "        writer.writerows([topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>hear, man, news, jag, bad, memory, future, mee...</td>\n",
       "      <td>When I heard a boy sing Ave Maria perfectly. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.1137</td>\n",
       "      <td>start, suddenly, fall, move, fear, water, run,...</td>\n",
       "      <td>I was at my house walking and suddenly spraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>make, big, situation, stand, open, girlfriend,...</td>\n",
       "      <td>I was at the field walking with my husband whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>beautiful, wonderful, world, sea, beauty, sun,...</td>\n",
       "      <td>I was astonished by watching the moon show up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>day, mother, father, family, end, young, reali...</td>\n",
       "      <td>1 My first son. 2º It was in October 1992. 3º ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>house, night, husband, hour, close, light, wal...</td>\n",
       "      <td>It made me feel I was in danger, it was at nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>remember, parent, talk, lot, room, cry, wait, ...</td>\n",
       "      <td>What made me feel that way was the moment I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>child, birth, son, daughter, hospital, bear, w...</td>\n",
       "      <td>The birth of my first son. / This happened whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>give, happy, love, find, doctor, pregnant, lea...</td>\n",
       "      <td>What: finding a person I did not see since a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>remember, parent, talk, lot, room, cry, wait, ...</td>\n",
       "      <td>Tandil, Indio Solaris gig, the worst was confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>start, suddenly, fall, move, fear, water, run,...</td>\n",
       "      <td>One time I had gone fishing when a fish jumped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0859</td>\n",
       "      <td>friend, home, watch, family, tv, amazed, girl,...</td>\n",
       "      <td>When I found out that my best friend was batte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>time, year, happen, good, thing, meet, partner...</td>\n",
       "      <td>I was in an amusement park with a group of fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>years_ago, visit, amazing, mountain, trip, cit...</td>\n",
       "      <td>My couple was cheating on me. Two years ago. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>time, year, happen, good, thing, meet, partner...</td>\n",
       "      <td>I was studying when my couple told me she was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>start, suddenly, fall, move, fear, water, run,...</td>\n",
       "      <td>It happened when I was a teenager, in a trip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>moment, place, immediately, back, surprise, fa...</td>\n",
       "      <td>A declaration about a sentimental situation of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>friend, home, watch, family, tv, amazed, girl,...</td>\n",
       "      <td>I saw the boyfriend of a friend kissing with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1289</td>\n",
       "      <td>day, mother, father, family, end, young, reali...</td>\n",
       "      <td>We were reunited with some friends, like every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>work, walk, leave, pass, receive, lose, decide...</td>\n",
       "      <td>I can mention three or four situations about i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>experience, time, feeling, sit, intense, stron...</td>\n",
       "      <td>Yes, when I began working at Cordobas municipa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>child, birth, son, daughter, hospital, bear, w...</td>\n",
       "      <td>The birth of my first son / 21 years ago / At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>work, walk, leave, pass, receive, lose, decide...</td>\n",
       "      <td>Many instances amaze me. The most important on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>years_ago, visit, amazing, mountain, trip, cit...</td>\n",
       "      <td>An image. Six years ago. I was lying down. My ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>years_ago, visit, amazing, mountain, trip, cit...</td>\n",
       "      <td>It was in the year 2009, I went on holidays wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>time, year, happen, good, thing, meet, partner...</td>\n",
       "      <td>I had read a book on esotericism about soul tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>make, long, ago, play, job, month, read, team,...</td>\n",
       "      <td>It happened when I was offered to conduct a TV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>start, suddenly, fall, move, fear, water, run,...</td>\n",
       "      <td>When I found out about my pregnancy. It happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>child, birth, son, daughter, hospital, bear, w...</td>\n",
       "      <td>It was the instant I saw my grandson for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>friend, home, watch, family, tv, amazed, girl,...</td>\n",
       "      <td>I was at home, an important notice about a fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1254</td>\n",
       "      <td>car, call, school, front, drive, leave, arrive...</td>\n",
       "      <td>Some years ago, I happened to see an accident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>hear, man, news, jag, bad, memory, future, mee...</td>\n",
       "      <td>We were going out of a food shop with my coupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>make, big, situation, stand, open, girlfriend,...</td>\n",
       "      <td>I was assaulted by someone holding a gun, when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>experience, time, feeling, sit, intense, stron...</td>\n",
       "      <td>An awe-inspiring experience was when I invoked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>feel, person, world, god, thing, eye, amazed, ...</td>\n",
       "      <td>It made me feel that, the desire of being with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>hear, man, news, jag, bad, memory, future, mee...</td>\n",
       "      <td>Anguish, the murder of J.F. Kennedy, year 1963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>feel, person, world, god, thing, eye, amazed, ...</td>\n",
       "      <td>Never, is it possible, cant you understand tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>life, people, live, great, reverence, change, ...</td>\n",
       "      <td>Holidays 2016/2. I was enjoying a ride on a bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>start, suddenly, fall, move, fear, water, run,...</td>\n",
       "      <td>In front of the Cataratas del Iguazú, when I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>experience, time, feeling, sit, intense, stron...</td>\n",
       "      <td>There were many occasions in which I felt amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>beautiful, wonderful, world, sea, beauty, sun,...</td>\n",
       "      <td>I was in Italy, near Etna Volcano, and saw the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>experience, time, feeling, sit, intense, stron...</td>\n",
       "      <td>My awe-related experience happened when we vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>work, walk, leave, pass, receive, lose, decide...</td>\n",
       "      <td>It was a very tough moment for me, the passing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>beautiful, wonderful, world, sea, beauty, sun,...</td>\n",
       "      <td>A beautiful landscape on a fine day looking ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0933</td>\n",
       "      <td>awe, feel, nature, small, sense, human, power,...</td>\n",
       "      <td>Watching the sunrise over Jaipur, India from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>car, call, school, front, drive, leave, arrive...</td>\n",
       "      <td>seize in san Francisco in October 1979 people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>beautiful, wonderful, world, sea, beauty, sun,...</td>\n",
       "      <td>Sunrise at Uluru (aka Ayers Rock). Alone, not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>house, night, husband, hour, close, light, wal...</td>\n",
       "      <td>i was with my husband in a helicopter flying i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>house, night, husband, hour, close, light, wal...</td>\n",
       "      <td>When I bought my new house and moved it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>life, people, live, great, reverence, change, ...</td>\n",
       "      <td>When my husband proposed to me. It was the big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>make, big, situation, stand, open, girlfriend,...</td>\n",
       "      <td>When I went bungee jumping. I was with a mate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>life, people, live, great, reverence, change, ...</td>\n",
       "      <td>i cant think of any thing, I have lived a shel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>awe, feel, nature, small, sense, human, power,...</td>\n",
       "      <td>Seeing Michael Jackson in concert was the most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>life, people, live, great, reverence, change, ...</td>\n",
       "      <td>seeing the great pyramids in egypt. the enormo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>car, call, school, front, drive, leave, arrive...</td>\n",
       "      <td>I WITNESSED A CAR ACCIDENT WTH MY FAMILY JUST ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>work, walk, leave, pass, receive, lose, decide...</td>\n",
       "      <td>i had taken a road trip with my best friend to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>work, walk, leave, pass, receive, lose, decide...</td>\n",
       "      <td>when I had a heart attack and ended up in hosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>car, call, school, front, drive, leave, arrive...</td>\n",
       "      <td>i was in my car driving. w hen a car was bakci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>awe, feel, nature, small, sense, human, power,...</td>\n",
       "      <td>I was maybe 13 or 14 years old, so it was 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>awe, feel, nature, small, sense, human, power,...</td>\n",
       "      <td>It was while hiking the mountains at 3500m ele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic  Perc_Contribution  \\\n",
       "0               2.0             0.1200   \n",
       "1              16.0             0.1137   \n",
       "2              12.0             0.0800   \n",
       "3               6.0             0.1468   \n",
       "4               5.0             0.0926   \n",
       "5               4.0             0.1005   \n",
       "6              10.0             0.0850   \n",
       "7              15.0             0.1241   \n",
       "8               9.0             0.0944   \n",
       "9              10.0             0.0932   \n",
       "10             16.0             0.0954   \n",
       "11              8.0             0.0859   \n",
       "12              3.0             0.0708   \n",
       "13              0.0             0.0744   \n",
       "14              3.0             0.0770   \n",
       "15             16.0             0.1042   \n",
       "16             19.0             0.0773   \n",
       "17              8.0             0.0763   \n",
       "18              5.0             0.1289   \n",
       "19             18.0             0.1055   \n",
       "20              1.0             0.0636   \n",
       "21             15.0             0.1460   \n",
       "22             18.0             0.1431   \n",
       "23              0.0             0.0770   \n",
       "24              0.0             0.0761   \n",
       "25              3.0             0.1117   \n",
       "26             14.0             0.1032   \n",
       "27             16.0             0.0884   \n",
       "28             15.0             0.1641   \n",
       "29              8.0             0.0942   \n",
       "..              ...                ...   \n",
       "76             13.0             0.1254   \n",
       "77              2.0             0.1102   \n",
       "78             12.0             0.1173   \n",
       "79              1.0             0.1003   \n",
       "83              7.0             0.0787   \n",
       "84              2.0             0.1165   \n",
       "86              7.0             0.0737   \n",
       "87             11.0             0.0818   \n",
       "92             16.0             0.0842   \n",
       "94              1.0             0.0881   \n",
       "98              6.0             0.0770   \n",
       "99              1.0             0.0878   \n",
       "101            18.0             0.0917   \n",
       "105             6.0             0.1274   \n",
       "109            17.0             0.0933   \n",
       "112            13.0             0.0842   \n",
       "115             6.0             0.1321   \n",
       "117             4.0             0.2104   \n",
       "120             4.0             0.0660   \n",
       "122            11.0             0.0645   \n",
       "123            12.0             0.0699   \n",
       "141            11.0             0.0809   \n",
       "148            17.0             0.0926   \n",
       "149            11.0             0.0811   \n",
       "164            13.0             0.0965   \n",
       "175            18.0             0.1152   \n",
       "188            18.0             0.0770   \n",
       "202            13.0             0.1478   \n",
       "216            17.0             0.1467   \n",
       "262            17.0             0.1068   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    hear, man, news, jag, bad, memory, future, mee...   \n",
       "1    start, suddenly, fall, move, fear, water, run,...   \n",
       "2    make, big, situation, stand, open, girlfriend,...   \n",
       "3    beautiful, wonderful, world, sea, beauty, sun,...   \n",
       "4    day, mother, father, family, end, young, reali...   \n",
       "5    house, night, husband, hour, close, light, wal...   \n",
       "6    remember, parent, talk, lot, room, cry, wait, ...   \n",
       "7    child, birth, son, daughter, hospital, bear, w...   \n",
       "8    give, happy, love, find, doctor, pregnant, lea...   \n",
       "9    remember, parent, talk, lot, room, cry, wait, ...   \n",
       "10   start, suddenly, fall, move, fear, water, run,...   \n",
       "11   friend, home, watch, family, tv, amazed, girl,...   \n",
       "12   time, year, happen, good, thing, meet, partner...   \n",
       "13   years_ago, visit, amazing, mountain, trip, cit...   \n",
       "14   time, year, happen, good, thing, meet, partner...   \n",
       "15   start, suddenly, fall, move, fear, water, run,...   \n",
       "16   moment, place, immediately, back, surprise, fa...   \n",
       "17   friend, home, watch, family, tv, amazed, girl,...   \n",
       "18   day, mother, father, family, end, young, reali...   \n",
       "19   work, walk, leave, pass, receive, lose, decide...   \n",
       "20   experience, time, feeling, sit, intense, stron...   \n",
       "21   child, birth, son, daughter, hospital, bear, w...   \n",
       "22   work, walk, leave, pass, receive, lose, decide...   \n",
       "23   years_ago, visit, amazing, mountain, trip, cit...   \n",
       "24   years_ago, visit, amazing, mountain, trip, cit...   \n",
       "25   time, year, happen, good, thing, meet, partner...   \n",
       "26   make, long, ago, play, job, month, read, team,...   \n",
       "27   start, suddenly, fall, move, fear, water, run,...   \n",
       "28   child, birth, son, daughter, hospital, bear, w...   \n",
       "29   friend, home, watch, family, tv, amazed, girl,...   \n",
       "..                                                 ...   \n",
       "76   car, call, school, front, drive, leave, arrive...   \n",
       "77   hear, man, news, jag, bad, memory, future, mee...   \n",
       "78   make, big, situation, stand, open, girlfriend,...   \n",
       "79   experience, time, feeling, sit, intense, stron...   \n",
       "83   feel, person, world, god, thing, eye, amazed, ...   \n",
       "84   hear, man, news, jag, bad, memory, future, mee...   \n",
       "86   feel, person, world, god, thing, eye, amazed, ...   \n",
       "87   life, people, live, great, reverence, change, ...   \n",
       "92   start, suddenly, fall, move, fear, water, run,...   \n",
       "94   experience, time, feeling, sit, intense, stron...   \n",
       "98   beautiful, wonderful, world, sea, beauty, sun,...   \n",
       "99   experience, time, feeling, sit, intense, stron...   \n",
       "101  work, walk, leave, pass, receive, lose, decide...   \n",
       "105  beautiful, wonderful, world, sea, beauty, sun,...   \n",
       "109  awe, feel, nature, small, sense, human, power,...   \n",
       "112  car, call, school, front, drive, leave, arrive...   \n",
       "115  beautiful, wonderful, world, sea, beauty, sun,...   \n",
       "117  house, night, husband, hour, close, light, wal...   \n",
       "120  house, night, husband, hour, close, light, wal...   \n",
       "122  life, people, live, great, reverence, change, ...   \n",
       "123  make, big, situation, stand, open, girlfriend,...   \n",
       "141  life, people, live, great, reverence, change, ...   \n",
       "148  awe, feel, nature, small, sense, human, power,...   \n",
       "149  life, people, live, great, reverence, change, ...   \n",
       "164  car, call, school, front, drive, leave, arrive...   \n",
       "175  work, walk, leave, pass, receive, lose, decide...   \n",
       "188  work, walk, leave, pass, receive, lose, decide...   \n",
       "202  car, call, school, front, drive, leave, arrive...   \n",
       "216  awe, feel, nature, small, sense, human, power,...   \n",
       "262  awe, feel, nature, small, sense, human, power,...   \n",
       "\n",
       "                                                     0  \n",
       "0    When I heard a boy sing Ave Maria perfectly. I...  \n",
       "1    I was at my house walking and suddenly spraine...  \n",
       "2    I was at the field walking with my husband whe...  \n",
       "3    I was astonished by watching the moon show up ...  \n",
       "4    1 My first son. 2º It was in October 1992. 3º ...  \n",
       "5    It made me feel I was in danger, it was at nig...  \n",
       "6    What made me feel that way was the moment I wa...  \n",
       "7    The birth of my first son. / This happened whe...  \n",
       "8    What: finding a person I did not see since a c...  \n",
       "9    Tandil, Indio Solaris gig, the worst was confi...  \n",
       "10   One time I had gone fishing when a fish jumped...  \n",
       "11   When I found out that my best friend was batte...  \n",
       "12   I was in an amusement park with a group of fri...  \n",
       "13   My couple was cheating on me. Two years ago. I...  \n",
       "14   I was studying when my couple told me she was ...  \n",
       "15   It happened when I was a teenager, in a trip t...  \n",
       "16   A declaration about a sentimental situation of...  \n",
       "17   I saw the boyfriend of a friend kissing with a...  \n",
       "18   We were reunited with some friends, like every...  \n",
       "19   I can mention three or four situations about i...  \n",
       "20   Yes, when I began working at Cordobas municipa...  \n",
       "21   The birth of my first son / 21 years ago / At ...  \n",
       "22   Many instances amaze me. The most important on...  \n",
       "23   An image. Six years ago. I was lying down. My ...  \n",
       "24   It was in the year 2009, I went on holidays wi...  \n",
       "25   I had read a book on esotericism about soul tr...  \n",
       "26   It happened when I was offered to conduct a TV...  \n",
       "27   When I found out about my pregnancy. It happen...  \n",
       "28   It was the instant I saw my grandson for the f...  \n",
       "29   I was at home, an important notice about a fam...  \n",
       "..                                                 ...  \n",
       "76   Some years ago, I happened to see an accident ...  \n",
       "77   We were going out of a food shop with my coupl...  \n",
       "78   I was assaulted by someone holding a gun, when...  \n",
       "79   An awe-inspiring experience was when I invoked...  \n",
       "83   It made me feel that, the desire of being with...  \n",
       "84   Anguish, the murder of J.F. Kennedy, year 1963...  \n",
       "86   Never, is it possible, cant you understand tha...  \n",
       "87   Holidays 2016/2. I was enjoying a ride on a bo...  \n",
       "92   In front of the Cataratas del Iguazú, when I c...  \n",
       "94   There were many occasions in which I felt amaz...  \n",
       "98   I was in Italy, near Etna Volcano, and saw the...  \n",
       "99   My awe-related experience happened when we vis...  \n",
       "101  It was a very tough moment for me, the passing...  \n",
       "105  A beautiful landscape on a fine day looking ac...  \n",
       "109  Watching the sunrise over Jaipur, India from a...  \n",
       "112  seize in san Francisco in October 1979 people ...  \n",
       "115  Sunrise at Uluru (aka Ayers Rock). Alone, not ...  \n",
       "117  i was with my husband in a helicopter flying i...  \n",
       "120           When I bought my new house and moved it.  \n",
       "122  When my husband proposed to me. It was the big...  \n",
       "123  When I went bungee jumping. I was with a mate ...  \n",
       "141  i cant think of any thing, I have lived a shel...  \n",
       "148  Seeing Michael Jackson in concert was the most...  \n",
       "149  seeing the great pyramids in egypt. the enormo...  \n",
       "164  I WITNESSED A CAR ACCIDENT WTH MY FAMILY JUST ...  \n",
       "175  i had taken a road trip with my best friend to...  \n",
       "188  when I had a heart attack and ended up in hosp...  \n",
       "202  i was in my car driving. w hen a car was bakci...  \n",
       "216  I was maybe 13 or 14 years old, so it was 1999...  \n",
       "262  It was while hiking the mountains at 3500m ele...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "sent_topics_outdf_grpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the frequency of each topic being the most dominant topic\n",
    "frequency = dict() # A dictionary. The key is the string of topic number. The item is the number of the appearence of this topic\n",
    "for topicIndex in range (0, num_of_topics):\n",
    "    frequency[str(topicIndex)] = sent_topics_outdf_grpd.get_group(float(topicIndex)).count().at['Dominant_Topic']\n",
    "\n",
    "    \n",
    "total_appearance = 0 # The number of narratives that have a dominant topic\n",
    "for i in range(0, num_of_topics):\n",
    "    total_appearance += frequency[str(i)]\n",
    "\n",
    "    \n",
    "frequency_rows = [None]*num_of_topics\n",
    "for i in range(0, num_of_topics):\n",
    "    temp = [None] * 4 # Each row always has 4 elements: Topic Number, Topic Keywords, Topic Appearance, Frequency\n",
    "    float_index = float(i) #The float version of the index for Dataframe Usage\n",
    "    temp[0] = float_index\n",
    "    temp[1] = sent_topics_outdf_grpd.get_group(float_index).iat[0, 2] \n",
    "    # 0 here means that the first rows(each topic will be the most domimant topic for at least one narrative by the nature of LDA)\n",
    "    # 2 here means we get the value at the third column, which is the Topic_Keywords\n",
    "    temp[2] = frequency[str(i)]\n",
    "    temp[3] = float(temp[2]) / float(total_appearance)\n",
    "    frequency_rows[i] = temp\n",
    "\n",
    "\n",
    "frequencyPath = outputPath + dataName + '_' + 'frequency_topics.csv' # Handle the output path\n",
    "\n",
    "# This part handles the header \n",
    "header = [None] * 4\n",
    "header[0] = \"Topic No.\"\n",
    "header[1] = \"Keywords\"\n",
    "header[2] = \"Appearance\"\n",
    "header[3] = \"Frequency\"\n",
    "\n",
    "with open(frequencyPath, 'w', newline='', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([header])\n",
    "    for frequency in frequency_rows:\n",
    "        writer.writerows([frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Index</th>\n",
       "      <th>Topic Contribution</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>years_ago, visit, amazing, mountain, trip, cit...</td>\n",
       "      <td>There is a temple in Maihar city,Satna distric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>experience, time, feeling, sit, intense, stron...</td>\n",
       "      <td>I was at a religious event i.e. at a church se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>hear, man, news, jag, bad, memory, future, mee...</td>\n",
       "      <td>Jag hade varit i Nepal i en månad med min dåva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>time, year, happen, good, thing, meet, partner...</td>\n",
       "      <td>It happened a few years ago. It was not a good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>house, night, husband, hour, close, light, wal...</td>\n",
       "      <td>I used to live in Co Meath in a rural area clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4080</td>\n",
       "      <td>day, mother, father, family, end, young, reali...</td>\n",
       "      <td>Seeing my mother hooked up to tubes after an o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>beautiful, wonderful, world, sea, beauty, sun,...</td>\n",
       "      <td>Stroll in the forest. Suddenly I had a feeling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>feel, person, world, god, thing, eye, amazed, ...</td>\n",
       "      <td>The time experienced the most awe of my life w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>friend, home, watch, family, tv, amazed, girl,...</td>\n",
       "      <td>What can cause me to be amazed is for example ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.2463</td>\n",
       "      <td>give, happy, love, find, doctor, pregnant, lea...</td>\n",
       "      <td>During 2015,overall all the months from Jan to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Index  Topic Contribution  \\\n",
       "0          0.0              0.3058   \n",
       "1          1.0              0.2274   \n",
       "2          2.0              0.5422   \n",
       "3          3.0              0.1731   \n",
       "4          4.0              0.3157   \n",
       "5          5.0              0.4080   \n",
       "6          6.0              0.2520   \n",
       "7          7.0              0.2526   \n",
       "8          8.0              0.2300   \n",
       "9          9.0              0.2463   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  years_ago, visit, amazing, mountain, trip, cit...   \n",
       "1  experience, time, feeling, sit, intense, stron...   \n",
       "2  hear, man, news, jag, bad, memory, future, mee...   \n",
       "3  time, year, happen, good, thing, meet, partner...   \n",
       "4  house, night, husband, hour, close, light, wal...   \n",
       "5  day, mother, father, family, end, young, reali...   \n",
       "6  beautiful, wonderful, world, sea, beauty, sun,...   \n",
       "7  feel, person, world, god, thing, eye, amazed, ...   \n",
       "8  friend, home, watch, family, tv, amazed, girl,...   \n",
       "9  give, happy, love, find, doctor, pregnant, lea...   \n",
       "\n",
       "                                                Text  \n",
       "0  There is a temple in Maihar city,Satna distric...  \n",
       "1  I was at a religious event i.e. at a church se...  \n",
       "2  Jag hade varit i Nepal i en månad med min dåva...  \n",
       "3  It happened a few years ago. It was not a good...  \n",
       "4  I used to live in Co Meath in a rural area clo...  \n",
       "5  Seeing my mother hooked up to tubes after an o...  \n",
       "6  Stroll in the forest. Suddenly I had a feeling...  \n",
       "7  The time experienced the most awe of my life w...  \n",
       "8  What can cause me to be amazed is for example ...  \n",
       "9  During 2015,overall all the months from Jan to...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "   \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic Index', \"Topic Contribution\", \"Keywords\", \"Text\"]\n",
    "\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontpath = 'font/SFCompact/SFCompactDisplay-Light.otf'# Use a local font.\n",
    "cloud = wordcloud.WordCloud(font_path =fontpath, width = 700, height = 600,\n",
    "                background_color = None, mode = 'RGBA', relative_scaling = 0.5, \n",
    "                            normalize_plurals = False) # The object for generating wordcloud.\n",
    "\n",
    "# The folder that stores these visulization.\n",
    "imgPath = outputPath + 'visualizations'\n",
    "if not os.path.exists(imgPath):\n",
    "    os.mkdir(imgPath)\n",
    "imgPath += '/'\n",
    "for topic in range(0, num_of_topics):\n",
    "    cloudict = dict()\n",
    "    for i in range(0, 15):\n",
    "        cloudict[allKeywords[topic][i]] = float(allPercentages[topic][i]) # Generate the frequency for the cloud object to use.\n",
    "    \n",
    "    img = cloud.generate_from_frequencies(cloudict, max_font_size=None)# Generate the image.\n",
    "    img.to_file(imgPath + 'Topic' + ' ' + str(topic) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataName is 'Overall'):\n",
    "    topicsArray = [None] * num_of_topics # An array that contains the distribution of countries for a topic.\n",
    "    for i in range(len(data)):\n",
    "        index = int(df_dominant_topic.iat[i, 1])\n",
    "        if topicsArray[index] is None:\n",
    "            topicsArray[index] = dict()\n",
    "        country = df.iat[i, 0]\n",
    "        if country in topicsArray[index]:\n",
    "            topicsArray[index][country] += 1\n",
    "        else:\n",
    "            topicsArray[index][country] = 1\n",
    "    \n",
    "    header = [None]\n",
    "    header[0] = 'Topic No.'\n",
    "    header = header + countryList\n",
    "    distributionRow = [None] * num_of_topics\n",
    "    for i in range(len(topicsArray)):\n",
    "        temp = [None] * len(header)\n",
    "        temp[0] = float(i)\n",
    "        j = 1\n",
    "        for country in countryList:\n",
    "            if country in topicsArray[i]:\n",
    "                temp[j] = topicsArray[i][country]\n",
    "            else:\n",
    "                temp[j] = 0\n",
    "            j += 1\n",
    "        distributionRow[i] = temp\n",
    "    \n",
    "    distributionPath = outputPath + dataName + '_' + 'distribution.csv'\n",
    "    with open(distributionPath, 'w', newline='', encoding = 'utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([header])\n",
    "        for distribution in distributionRow:\n",
    "            writer.writerows([distribution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina\n",
      "105\n",
      "Australia\n",
      "105\n",
      "Austria\n",
      "105\n",
      "Brazil\n",
      "118\n",
      "Canada\n",
      "105\n",
      "Chile\n",
      "105\n",
      "China\n",
      "105\n",
      "France\n",
      "105\n",
      "Germany\n",
      "104\n",
      "India\n",
      "130\n",
      "Indonesia\n",
      "105\n",
      "Ireland\n",
      "105\n",
      "Japan\n",
      "104\n",
      "Korea\n",
      "105\n",
      "Mexico\n",
      "105\n",
      "Netherlands\n",
      "105\n",
      "Norway\n",
      "104\n",
      "Russia\n",
      "105\n",
      "Singapore\n",
      "105\n",
      "South_Africa\n",
      "104\n",
      "Spain\n",
      "104\n",
      "Sweden\n",
      "105\n",
      "Switzerland\n",
      "105\n",
      "Turkey\n",
      "104\n",
      "UK\n",
      "106\n",
      "USA\n",
      "105\n",
      "2763\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for country in countryList:\n",
    "    print(country)\n",
    "    print(df.Country.value_counts()[country])\n",
    "    total += df.Country.value_counts()[country]\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2764"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
